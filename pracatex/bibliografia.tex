\begin{thebibliography}{99}
\addcontentsline{toc}{chapter}{Bibliografia}
\bibitem{hybrid}{Batista G., Prati R. C., Monard M. C., A study of the behavior of several methods for	balancing machine learning training data. ACM SIGKDD Explorations Newsletter, 2004.}
\bibitem{Bishop}{Bishop C. M., Neural Networks for Pattern Recognition. Claredon press. Oxford, 1995.}
\bibitem{baggingc}{Breiman L., Bagging Predictors, Machine Learning, 1996.}
\bibitem{randomFc}{Breiman L., Random Forests. Machine Learning, 2001.}
\bibitem{smotec}{Chawla N. V., Bowyer K. W., Hall L. O., Kegelmeyer W. P., SMOTE: synthetic minority over-sampling technique, 2002.}
\bibitem{adaboostc}{Freund Y., Schapire R. E., A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, 1996.}
\bibitem{boostingc}{Freund Y., Schapire R. E., Experiments with a~New Boosting Algorithm. Proceedings of the 13th International Conference on Machine Learning, Bari, 1996.}
\bibitem{adasync}{He H., Bai Y., Garcia E. A., Li S., ADASYN: Adaptive synthetic sampling approach	for imbalanced learning, 2008.}
\bibitem{Garcia}{He H., Garcia E. A., Learning from imbalanced data. IEEE Transactions on Data and Knowledge Engineering, 2009.}
\bibitem{hyper}{Hu, F., Liu X., Dai J., Yu H., A Novel Algorithm for Imbalance Data Classification Based on Neighborhood Hypergraph.}
\bibitem{boosting}{Long P., Servedio R., Random Classification Noise Defeats All Convex Potential Boosters.}
\bibitem{KubatMatwin}{Kubat M., Matwin S., Addresing the curse of imbalanced training sets: one-side selection.}
\bibitem{ncr}{Laurikkala J., Improving identification of difficult small classes by balancing class distribution. Technical report, University of Tampere, 2001.}
\bibitem{przykladyklas}{Napierala K., Stefanowski J., Identification of Different Types of Minority Class Examples in Imbalanced Data. In: E. Corchado, V. Snasel, A.Abraham, M. Wozniak et al. (eds): Hybrid Artificial Intelligence Systems, Proc. 7th Int Conference HAIS 2012.}
\bibitem{StefImbalanced}{Stefanowski J., Dealing with Data Difficulty Factors while Learning from Imbalanced Data.}

\bibitem{tomelinksc}{Tomek I., Two modifications of CNN. IEEE Transactions on Systems Man and Communications, 1976.}
\bibitem{enn}{Wilson D. L., Asymptotic properties of nearest neighbor rules using edited data. IEEE	Transactions on Systems, Man, and Communications, 1972.}
\bibitem{stackingc}{Wolpert D., Stacked Generalization, Neural Networks, 1992.}

\bibitem{mlxtend}{S. Raschka. Mlxtend (machine learning extensions), https://github.com/rasbt/mlxtend}
\bibitem{gocardless}{Hockham N., Machine learning with imbalanced data sets https://www.youtube.com/watch?v=X9MZtvvQDR4}
\bibitem{uci}{The UCI Machine Learning Repository, https://archive.ics.uci.edu/ml/}
\bibitem{python}{Python Software Foundation. https://www.python.org/}
\bibitem{scikit}{scikit-learn Machine Learning in Python. http://scikit-learn.org/}
\bibitem{imlearn}{imbalanced-learn. http://contrib.scikit-learn.org/imbalanced-learn/}


\end{thebibliography}
\listoffigures



\addcontentsline{toc}{section}{Spis rysunk√≥w}
\listoftables

\addcontentsline{toc}{section}{Spis tabel}